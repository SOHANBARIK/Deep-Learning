{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN7emxdb8y47/I+uIf9HOeI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AV-RykzBeZy",
        "outputId": "5347642b-050a-4e81-863c-4807987cabd3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nkuG7iNa5zo4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences,to_categorical\n",
        "from tensorflow.keras.layers import Embedding,Dropout,Layer,LayerNormalization,Dense\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "  with open(file_path,'r',encoding='utf-8') as f:\n",
        "    text= f.read()\n",
        "  return text\n",
        "file_path='hp1.txt'\n",
        "text= load_data(file_path).lower() # This is our vobaculary\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer= Tokenizer(oov_token='')\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words=len(tokenizer.word_index)+1\n",
        "\n",
        "# Convert text to sequences\n",
        "tokens=tokenizer.texts_to_sequences([text])[0] # created an embedding\n",
        "seq_len= 50\n",
        "\n",
        "# First seq_length tokens (input): Used for training the model.\n",
        "# Last token (target): Used as the label the model tries to predict.\n",
        "# so total of (50 + 1) in one input_sequence index\n",
        "input_sequences=[]\n",
        "for i in range(seq_len,len(tokens)):\n",
        "  input_sequences.append(tokens[i-seq_len:i+1])\n",
        "\n",
        "#print(input_sequences[0])\n",
        "\n",
        "# Pad sequences and split inputs/targets\n",
        "# after this X will have inputs and y will have label for those inputs\n",
        "input_sequences = np.array(pad_sequences(input_sequences,maxlen=seq_len+1,padding='pre'))\n",
        "X,y= input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "#Encoding\n",
        "#One hot encoding the labels(y), Note there ae other ways for encoding like pre-trained word2vec encoding and so on\n",
        "y=tf.keras.utils.to_categorical(y, num_classes=total_words)"
      ],
      "metadata": {
        "id": "e69YNeuEBWFs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgJEF8a3HhwV",
        "outputId": "41796a71-3e80-4904-b2ff-7b2906f2f6d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6663"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens # 1D array hai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bpa0PJDMKk7A",
        "outputId": "a55938f5-76c0-41c8-f871-3c15a6543be2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2162,\n",
              " 3680,\n",
              " 4,\n",
              " 274,\n",
              " 224,\n",
              " 8,\n",
              " 651,\n",
              " 332,\n",
              " 652,\n",
              " 535,\n",
              " 35,\n",
              " 1268,\n",
              " 5,\n",
              " 164,\n",
              " 20,\n",
              " 21,\n",
              " 35,\n",
              " 1586,\n",
              " 973,\n",
              " 1587,\n",
              " 14,\n",
              " 69,\n",
              " 157,\n",
              " 21,\n",
              " 35,\n",
              " 2,\n",
              " 141,\n",
              " 128,\n",
              " 653,\n",
              " 789,\n",
              " 5,\n",
              " 32,\n",
              " 1588,\n",
              " 12,\n",
              " 169,\n",
              " 490,\n",
              " 110,\n",
              " 1416,\n",
              " 142,\n",
              " 21,\n",
              " 68,\n",
              " 55,\n",
              " 909,\n",
              " 25,\n",
              " 505,\n",
              " 1788,\n",
              " 151,\n",
              " 224,\n",
              " 10,\n",
              " 2,\n",
              " 2701,\n",
              " 8,\n",
              " 6,\n",
              " 2702,\n",
              " 275,\n",
              " 2703,\n",
              " 140,\n",
              " 183,\n",
              " 1417,\n",
              " 7,\n",
              " 10,\n",
              " 6,\n",
              " 394,\n",
              " 3681,\n",
              " 333,\n",
              " 25,\n",
              " 491,\n",
              " 191,\n",
              " 593,\n",
              " 974,\n",
              " 7,\n",
              " 131,\n",
              " 36,\n",
              " 6,\n",
              " 69,\n",
              " 233,\n",
              " 1418,\n",
              " 274,\n",
              " 224,\n",
              " 10,\n",
              " 975,\n",
              " 4,\n",
              " 2704,\n",
              " 4,\n",
              " 17,\n",
              " 343,\n",
              " 689,\n",
              " 2,\n",
              " 594,\n",
              " 3682,\n",
              " 8,\n",
              " 593,\n",
              " 140,\n",
              " 159,\n",
              " 12,\n",
              " 69,\n",
              " 1789,\n",
              " 22,\n",
              " 46,\n",
              " 910,\n",
              " 53,\n",
              " 157,\n",
              " 8,\n",
              " 60,\n",
              " 89,\n",
              " 2705,\n",
              " 63,\n",
              " 1419,\n",
              " 3683,\n",
              " 2163,\n",
              " 18,\n",
              " 2,\n",
              " 2706,\n",
              " 2,\n",
              " 287,\n",
              " 17,\n",
              " 6,\n",
              " 356,\n",
              " 1090,\n",
              " 275,\n",
              " 93,\n",
              " 4,\n",
              " 12,\n",
              " 49,\n",
              " 1589,\n",
              " 38,\n",
              " 10,\n",
              " 73,\n",
              " 2707,\n",
              " 146,\n",
              " 1269,\n",
              " 2,\n",
              " 287,\n",
              " 17,\n",
              " 383,\n",
              " 21,\n",
              " 276,\n",
              " 26,\n",
              " 21,\n",
              " 690,\n",
              " 17,\n",
              " 6,\n",
              " 691,\n",
              " 4,\n",
              " 49,\n",
              " 1590,\n",
              " 976,\n",
              " 10,\n",
              " 20,\n",
              " 2164,\n",
              " 107,\n",
              " 3684,\n",
              " 11,\n",
              " 21,\n",
              " 55,\n",
              " 87,\n",
              " 21,\n",
              " 57,\n",
              " 1591,\n",
              " 11,\n",
              " 45,\n",
              " 327,\n",
              " 179,\n",
              " 29,\n",
              " 47,\n",
              " 2,\n",
              " 911,\n",
              " 274,\n",
              " 117,\n",
              " 10,\n",
              " 274,\n",
              " 2708,\n",
              " 731,\n",
              " 26,\n",
              " 21,\n",
              " 221,\n",
              " 564,\n",
              " 31,\n",
              " 790,\n",
              " 234,\n",
              " 12,\n",
              " 692,\n",
              " 274,\n",
              " 224,\n",
              " 2165,\n",
              " 46,\n",
              " 55,\n",
              " 36,\n",
              " 6,\n",
              " 731,\n",
              " 142,\n",
              " 60,\n",
              " 731,\n",
              " 4,\n",
              " 60,\n",
              " 165,\n",
              " 31,\n",
              " 249,\n",
              " 3685,\n",
              " 35,\n",
              " 22,\n",
              " 3686,\n",
              " 22,\n",
              " 11,\n",
              " 10,\n",
              " 848,\n",
              " 5,\n",
              " 32,\n",
              " 2,\n",
              " 287,\n",
              " 2166,\n",
              " 5,\n",
              " 87,\n",
              " 37,\n",
              " 2,\n",
              " 2706,\n",
              " 107,\n",
              " 164,\n",
              " 45,\n",
              " 2,\n",
              " 911,\n",
              " 625,\n",
              " 12,\n",
              " 2,\n",
              " 506,\n",
              " 2,\n",
              " 287,\n",
              " 198,\n",
              " 20,\n",
              " 2,\n",
              " 911,\n",
              " 17,\n",
              " 6,\n",
              " 356,\n",
              " 1090,\n",
              " 114,\n",
              " 26,\n",
              " 21,\n",
              " 17,\n",
              " 94,\n",
              " 113,\n",
              " 225,\n",
              " 23,\n",
              " 43,\n",
              " 146,\n",
              " 10,\n",
              " 228,\n",
              " 165,\n",
              " 492,\n",
              " 31,\n",
              " 977,\n",
              " 2,\n",
              " 911,\n",
              " 173,\n",
              " 21,\n",
              " 55,\n",
              " 180,\n",
              " 93,\n",
              " 2709,\n",
              " 25,\n",
              " 6,\n",
              " 2710,\n",
              " 56,\n",
              " 20,\n",
              " 62,\n",
              " 151,\n",
              " 4,\n",
              " 274,\n",
              " 224,\n",
              " 978,\n",
              " 30,\n",
              " 18,\n",
              " 2,\n",
              " 2711,\n",
              " 791,\n",
              " 2167,\n",
              " 293,\n",
              " 912,\n",
              " 2168,\n",
              " 38,\n",
              " 10,\n",
              " 249,\n",
              " 47,\n",
              " 2,\n",
              " 2712,\n",
              " 792,\n",
              " 318,\n",
              " 5,\n",
              " 1790,\n",
              " 20,\n",
              " 490,\n",
              " 4,\n",
              " 1416,\n",
              " 236,\n",
              " 107,\n",
              " 732,\n",
              " 32,\n",
              " 1791,\n",
              " 28,\n",
              " 63,\n",
              " 2,\n",
              " 1792,\n",
              " 151,\n",
              " 224,\n",
              " 2169,\n",
              " 22,\n",
              " 7,\n",
              " 654,\n",
              " 29,\n",
              " 13,\n",
              " 395,\n",
              " 2170,\n",
              " 3687,\n",
              " 31,\n",
              " 396,\n",
              " 4,\n",
              " 274,\n",
              " 224,\n",
              " 3688,\n",
              " 173,\n",
              " 1592,\n",
              " 22,\n",
              " 46,\n",
              " 3689,\n",
              " 6,\n",
              " 1793,\n",
              " 93,\n",
              " 48,\n",
              " 13,\n",
              " 263,\n",
              " 979,\n",
              " 980,\n",
              " 8,\n",
              " 33,\n",
              " 384,\n",
              " 6,\n",
              " 233,\n",
              " 2713,\n",
              " 277,\n",
              " 2171,\n",
              " 278,\n",
              " 2,\n",
              " 357,\n",
              " 19,\n",
              " 299,\n",
              " 278,\n",
              " 1420,\n",
              " 151,\n",
              " 224,\n",
              " 654,\n",
              " 30,\n",
              " 13,\n",
              " 3690,\n",
              " 3691,\n",
              " 274,\n",
              " 224,\n",
              " 18,\n",
              " 2,\n",
              " 1794,\n",
              " 4,\n",
              " 189,\n",
              " 5,\n",
              " 2172,\n",
              " 93,\n",
              " 165,\n",
              " 1270,\n",
              " 26,\n",
              " 1795,\n",
              " 142,\n",
              " 93,\n",
              " 10,\n",
              " 79,\n",
              " 733,\n",
              " 6,\n",
              " 2173,\n",
              " 4,\n",
              " 981,\n",
              " 13,\n",
              " 3692,\n",
              " 19,\n",
              " 2,\n",
              " 655,\n",
              " 2714,\n",
              " 2715,\n",
              " 3,\n",
              " 2716,\n",
              " 151,\n",
              " 224,\n",
              " 22,\n",
              " 7,\n",
              " 144,\n",
              " 2,\n",
              " 155,\n",
              " 7,\n",
              " 58,\n",
              " 48,\n",
              " 13,\n",
              " 565,\n",
              " 4,\n",
              " 2174,\n",
              " 29,\n",
              " 8,\n",
              " 651,\n",
              " 3693,\n",
              " 535,\n",
              " 11,\n",
              " 10,\n",
              " 18,\n",
              " 2,\n",
              " 431,\n",
              " 8,\n",
              " 2,\n",
              " 506,\n",
              " 20,\n",
              " 7,\n",
              " 384,\n",
              " 2,\n",
              " 96,\n",
              " 656,\n",
              " 8,\n",
              " 85,\n",
              " 1796,\n",
              " 16,\n",
              " 6,\n",
              " 466,\n",
              " 1593,\n",
              " 6,\n",
              " 1797,\n",
              " 31,\n",
              " 6,\n",
              " 288,\n",
              " 151,\n",
              " 224,\n",
              " 55,\n",
              " 982,\n",
              " 37,\n",
              " 7,\n",
              " 17,\n",
              " 225,\n",
              " 16,\n",
              " 72,\n",
              " 7,\n",
              " 983,\n",
              " 13,\n",
              " 118,\n",
              " 78,\n",
              " 5,\n",
              " 100,\n",
              " 115,\n",
              " 38,\n",
              " 10,\n",
              " 6,\n",
              " 1798,\n",
              " 466,\n",
              " 344,\n",
              " 18,\n",
              " 2,\n",
              " 431,\n",
              " 8,\n",
              " 652,\n",
              " 535,\n",
              " 26,\n",
              " 38,\n",
              " 158,\n",
              " 6,\n",
              " 1797,\n",
              " 12,\n",
              " 432,\n",
              " 37,\n",
              " 57,\n",
              " 7,\n",
              " 36,\n",
              " 50,\n",
              " 793,\n",
              " 8,\n",
              " 11,\n",
              " 217,\n",
              " 36,\n",
              " 50,\n",
              " 6,\n",
              " 2175,\n",
              " 8,\n",
              " 2,\n",
              " 433,\n",
              " 151,\n",
              " 224,\n",
              " 1594,\n",
              " 4,\n",
              " 328,\n",
              " 19,\n",
              " 2,\n",
              " 466,\n",
              " 11,\n",
              " 328,\n",
              " 39,\n",
              " 22,\n",
              " 151,\n",
              " 224,\n",
              " 1421,\n",
              " 78,\n",
              " 2,\n",
              " 431,\n",
              " 4,\n",
              " 30,\n",
              " 2,\n",
              " 1271,\n",
              " 7,\n",
              " 507,\n",
              " 2,\n",
              " 466,\n",
              " 12,\n",
              " 13,\n",
              " 257,\n",
              " 11,\n",
              " 10,\n",
              " 79,\n",
              " 1593,\n",
              " 2,\n",
              " 656,\n",
              " 20,\n",
              " 15,\n",
              " 652,\n",
              " 535,\n",
              " 16,\n",
              " 73,\n",
              " 97,\n",
              " 19,\n",
              " 2,\n",
              " 656,\n",
              " 1422,\n",
              " 111,\n",
              " 289,\n",
              " 3694,\n",
              " 110,\n",
              " 3695,\n",
              " 151,\n",
              " 224,\n",
              " 308,\n",
              " 185,\n",
              " 6,\n",
              " 174,\n",
              " 1423,\n",
              " 4,\n",
              " 199,\n",
              " 2,\n",
              " 466,\n",
              " 29,\n",
              " 8,\n",
              " 13,\n",
              " 245,\n",
              " 22,\n",
              " 7,\n",
              " 1421,\n",
              " 170,\n",
              " 1595,\n",
              " 7,\n",
              " 124,\n",
              " 8,\n",
              " 249,\n",
              " 446,\n",
              " 6,\n",
              " 233,\n",
              " 1799,\n",
              " 8,\n",
              " 1417,\n",
              " 7,\n",
              " 10,\n",
              " 913,\n",
              " 5,\n",
              " 59,\n",
              " 20,\n",
              " 206,\n",
              " 26,\n",
              " 18,\n",
              " 2,\n",
              " 914,\n",
              " 8,\n",
              " 1595,\n",
              " 1417,\n",
              " 35,\n",
              " 2176,\n",
              " 29,\n",
              " 8,\n",
              " 13,\n",
              " 245,\n",
              " 74,\n",
              " 85,\n",
              " 329,\n",
              " 22,\n",
              " 7,\n",
              " 264,\n",
              " 12,\n",
              " 2,\n",
              " 594,\n",
              " 366,\n",
              " 2177,\n",
              " 2178,\n",
              " 7,\n",
              " 111,\n",
              " 397,\n",
              " 1091,\n",
              " 20,\n",
              " 38,\n",
              " 166,\n",
              " 5,\n",
              " 32,\n",
              " 6,\n",
              " 250,\n",
              " 8,\n",
              " 1596,\n",
              " 1272,\n",
              " 128,\n",
              " 47,\n",
              " 128,\n",
              " 12,\n",
              " 1597,\n",
              " 151,\n",
              " 224,\n",
              " 111,\n",
              " 1591,\n",
              " 128,\n",
              " 64,\n",
              " 1272,\n",
              " 12,\n",
              " 419,\n",
              " 1167,\n",
              " 16,\n",
              " 2,\n",
              " 3696,\n",
              " 14,\n",
              " 190,\n",
              " 18,\n",
              " 1092,\n",
              " 128,\n",
              " 7,\n",
              " 657,\n",
              " 43,\n",
              " 10,\n",
              " 153,\n",
              " 566,\n",
              " 309,\n",
              " 2717,\n",
              " 7,\n",
              " 3697,\n",
              " 13,\n",
              " 734,\n",
              " 18,\n",
              " 2,\n",
              " 2718,\n",
              " 3698,\n",
              " 4,\n",
              " 13,\n",
              " 108,\n",
              " 258,\n",
              " 18,\n",
              " 6,\n",
              " 2719,\n",
              " 8,\n",
              " 319,\n",
              " 3699,\n",
              " 344,\n",
              " 320,\n",
              " 567,\n",
              " 74,\n",
              " 21,\n",
              " 35,\n",
              " 1800,\n",
              " 1424,\n",
              " 536,\n",
              " 151,\n",
              " 224,\n",
              " 10,\n",
              " 3700,\n",
              " 5,\n",
              " 70,\n",
              " 20,\n",
              " 6,\n",
              " 1168,\n",
              " 8,\n",
              " 33,\n",
              " 420,\n",
              " 1092,\n",
              " 19,\n",
              " 28,\n",
              " 265,\n",
              " 20,\n",
              " 333,\n",
              " 17,\n",
              " 5,\n",
              " 32,\n",
              " 1598,\n",
              " 126,\n",
              " 7,\n",
              " 10,\n",
              " 4,\n",
              " 626,\n",
              " 84,\n",
              " 1425,\n",
              " 367,\n",
              " 212,\n",
              " 2,\n",
              " 1599,\n",
              " 8,\n",
              " 23,\n",
              " 26,\n",
              " 72,\n",
              " 11,\n",
              " 1426,\n",
              " 151,\n",
              " 224,\n",
              " 20,\n",
              " 43,\n",
              " 10,\n",
              " 1169,\n",
              " 153,\n",
              " 2179,\n",
              " 3701,\n",
              " 3702,\n",
              " 128,\n",
              " 35,\n",
              " 849,\n",
              " 2180,\n",
              " 31,\n",
              " 3703,\n",
              " 20,\n",
              " 107,\n",
              " 32,\n",
              " 11,\n",
              " 2,\n",
              " 2177,\n",
              " 493,\n",
              " 18,\n",
              " 4,\n",
              " 6,\n",
              " 222,\n",
              " 411,\n",
              " 281,\n",
              " 151,\n",
              " 224,\n",
              " 625,\n",
              " 12,\n",
              " 2,\n",
              " 2703,\n",
              " 2181,\n",
              " 250,\n",
              " 13,\n",
              " 245,\n",
              " 39,\n",
              " 18,\n",
              " 1417,\n",
              " 151,\n",
              " 224,\n",
              " 300,\n",
              " 264,\n",
              " 25,\n",
              " 13,\n",
              " 39,\n",
              " 5,\n",
              " 2,\n",
              " 357,\n",
              " 12,\n",
              " 13,\n",
              " 1600,\n",
              " 18,\n",
              " 2,\n",
              " 3704,\n",
              " 226,\n",
              " 45,\n",
              " 7,\n",
              " 221,\n",
              " 7,\n",
              " 227,\n",
              " 36,\n",
              " 179,\n",
              " 11,\n",
              " 1273,\n",
              " 5,\n",
              " 2182,\n",
              " 18,\n",
              " 1417,\n",
              " 20,\n",
              " 366,\n",
              " 7,\n",
              " 55,\n",
              " 70,\n",
              " 2,\n",
              " 508,\n",
              " 1601,\n",
              " 278,\n",
              " 12,\n",
              " 2720,\n",
              " 1602,\n",
              " 138,\n",
              " 128,\n",
              " 61,\n",
              " 12,\n",
              " 2,\n",
              " 506,\n",
              " 131,\n",
              " 21,\n",
              " 509,\n",
              " 4,\n",
              " 3705,\n",
              " 213,\n",
              " 2183,\n",
              " 22,\n",
              " 277,\n",
              " 186,\n",
              " 277,\n",
              " 1170,\n",
              " 1427,\n",
              " 395,\n",
              " 8,\n",
              " 33,\n",
              " 17,\n",
              " 94,\n",
              " 225,\n",
              " 84,\n",
              " 277,\n",
              " 113,\n",
              " 19,\n",
              " 2184,\n",
              " 151,\n",
              " 224,\n",
              " 537,\n",
              " 17,\n",
              " 6,\n",
              " 1586,\n",
              " 973,\n",
              " 277,\n",
              " 693,\n",
              " 366,\n",
              " 7,\n",
              " 694,\n",
              " 19,\n",
              " 368,\n",
              " 538,\n",
              " 128,\n",
              " 7,\n",
              " 183,\n",
              " 790,\n",
              " 658,\n",
              " 1428,\n",
              " 2721,\n",
              " 4,\n",
              " 385,\n",
              " 6,\n",
              " 181,\n",
              " 101,\n",
              " 7,\n",
              " 10,\n",
              " 12,\n",
              " 6,\n",
              " 69,\n",
              " 165,\n",
              " 1603,\n",
              " 266,\n",
              " 2722,\n",
              " 62,\n",
              " 7,\n",
              " 124,\n",
              " 81,\n",
              " 3706,\n",
              " 13,\n",
              " 494,\n",
              " 4,\n",
              " 850,\n",
              " 310,\n",
              " 2,\n",
              " 1271,\n",
              " 5,\n",
              " 851,\n",
              " 185,\n",
              " 6,\n",
              " 2723,\n",
              " 42,\n",
              " 2,\n",
              " 3707,\n",
              " 81,\n",
              " 31,\n",
              " 510,\n",
              " 28,\n",
              " 47,\n",
              " 2,\n",
              " 128,\n",
              " 12,\n",
              " 1597,\n",
              " 266,\n",
              " 7,\n",
              " 447,\n",
              " 6,\n",
              " 2724,\n",
              " 8,\n",
              " 33,\n",
              " 121,\n",
              " 5,\n",
              " 2,\n",
              " 3708,\n",
              " 7,\n",
              " 1604,\n",
              " 33,\n",
              " 1274,\n",
              " 22,\n",
              " 7,\n",
              " 447,\n",
              " 7,\n",
              " 55,\n",
              " 52,\n",
              " 265,\n",
              " 26,\n",
              " 21,\n",
              " 183,\n",
              " 23,\n",
              " 2725,\n",
              " 43,\n",
              " 2726,\n",
              " 35,\n",
              " 1800,\n",
              " 1424,\n",
              " 114,\n",
              " 4,\n",
              " 7,\n",
              " 111,\n",
              " 70,\n",
              " 6,\n",
              " 984,\n",
              " 2180,\n",
              " 3709,\n",
              " 11,\n",
              " 10,\n",
              " 18,\n",
              " 13,\n",
              " 127,\n",
              " 39,\n",
              " 278,\n",
              " 33,\n",
              " 1275,\n",
              " 6,\n",
              " 233,\n",
              " 3710,\n",
              " 12,\n",
              " 6,\n",
              " 985,\n",
              " 20,\n",
              " 7,\n",
              " 305,\n",
              " 6,\n",
              " 222,\n",
              " 448,\n",
              " 8,\n",
              " 37,\n",
              " 21,\n",
              " 35,\n",
              " 279,\n",
              " 290,\n",
              " 911,\n",
              " 207,\n",
              " 95,\n",
              " 207,\n",
              " 37,\n",
              " 24,\n",
              " 160,\n",
              " 119,\n",
              " 1429,\n",
              " 16,\n",
              " 369,\n",
              " 49,\n",
              " 1090,\n",
              " 9,\n",
              " 119,\n",
              " 151,\n",
              " 224,\n",
              " 386,\n",
              " 449,\n",
              " 976,\n",
              " 1801,\n",
              " 23,\n",
              " 7,\n",
              " 65,\n",
              " 39,\n",
              " 19,\n",
              " 2,\n",
              " 3711,\n",
              " 22,\n",
              " 45,\n",
              " 7,\n",
              " 276,\n",
              " 5,\n",
              " 164,\n",
              " 85,\n",
              " 5,\n",
              " 33,\n",
              " 26,\n",
              " 124,\n",
              " 311,\n",
              " 8,\n",
              " 11,\n",
              " 7,\n",
              " 2185,\n",
              " 39,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbyM-uusL2rQ",
        "outputId": "63b6c714-2f01-491a-8168-fccd4b84e2b6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81022"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OXlnWxJ6NUGn",
        "outputId": "90b0d0b7-1dad-4788-b11b-1444de0b92ab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80972, 51)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSdyliSYXdXm",
        "outputId": "fedf0781-f3e9-4c50-d969-873d473bc838"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2162, 3680,    4, ...,   10,    2, 2701],\n",
              "       [3680,    4,  274, ...,    2, 2701,    8],\n",
              "       [   4,  274,  224, ..., 2701,    8,    6],\n",
              "       ...,\n",
              "       [3276,  186,   88, ..., 1219,   25,   93],\n",
              "       [ 186,   88,  112, ...,   25,   93,   43],\n",
              "       [  88,  112, 1281, ...,   93,   43, 1204]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCalXaL8Ndqf",
        "outputId": "20ba5b0c-399a-4466-d7d2-a4661d296c96"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80972, 6663)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core of the Transformer model"
      ],
      "metadata": {
        "id": "VFImq8hyjH5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Embedding, Dense, LayerNormalization, Dropout\n",
        "\n",
        "class MultiHeadAttention(Layer):\n",
        "  def __init__(self,embed_dim,num_heads):\n",
        "    super(MultiHeadAttention,self).__init__() # This line calls the constructor of the parent class (Layer)\n",
        "    self.num_heads=num_heads # example - 8\n",
        "\n",
        "    self.embed_dim= embed_dim # example - 512\n",
        "    # embed_dim = dimension of Q, K and V before splitting into multiple dimensions\n",
        "    # It is same as total dimension of the input embeddings (word embeddings)\n",
        "    self.projection_dim= embed_dim // num_heads # Size of Each Attention Head's Subspace\n",
        "    # Each head gets a smaller subspace of the embedding dimension\n",
        "    #example - 64\n",
        "\n",
        "    # Fully connected (dense) layers that project the input into Q,K,V\n",
        "    # These layers map the input embeddings to the same embed_dim\n",
        "    # These layers will be reshaped/split later to split across attention heads\n",
        "    # A single large matrix multiplication is more efficient than many small ones\n",
        "    # GPUs love large matrix multiplication because they are optimized for parallel computation\n",
        "    # This allows TF/Keras to efficiently batch the computation, leveraging better GPU memory utilization\n",
        "\n",
        "    self.query_dense = Dense(embed_dim) # Q determines \"what to focus on/what to search\"\n",
        "    self.key_dense = Dense(embed_dim) # K acts as \"labels\" to matched with queries\n",
        "    self.value_dense = Dense(embed_dim) # V holds the actual information\n",
        "\n",
        "    self.combine_heads = Dense(embed_dim)\n",
        "    # After multi-head attention is applied, the outputs from all heads are concatenated back into embed_dim\n",
        "\n",
        "  def attention(self, query, key, value):\n",
        "    #projection_dim=self.projection_dim\n",
        "    scores= tf.matmul(query,key, transpose_b=True)\n",
        "    scores/=tf.math.sqrt(tf.cast(self.projection_dim, tf.float32)) # converting integer to float32 tensor # tensor means multidimensional array\n",
        "    # attention scores\n",
        "    attention_probs= tf.nn.softmax(scores, axis=-1) # how much attention each token should give to other tokens\n",
        "    # The higher the score, the more focus that token gets\n",
        "    # Softmax should be applied along the keys(i.e, across the last dimension of the scores matrix)\n",
        "    # Each row corresponds to a query token attending to all key tokens\n",
        "    # This ensures that each query distributes it's attention to all keys properly\n",
        "    # Each row sums to 1\n",
        "\n",
        "    return tf.matmul(attention_probs, value), attention_probs\n",
        "  # x - query,key,value with shape - (batch_size,seq_length,embed_dim)\n",
        "  # batch_size - number of sequences being processed together in parallel(for batch processing)\n",
        "  def split_heads(self,x, batch_size):\n",
        "    x= tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "    return tf.transpose(x, perm= [0,2,1,3])\n",
        "    # before transpose - (batch_size, seq_len, num_heads, projection_dim)\n",
        "    # after transpose - (batch_size, num_heads,seq_len, projection_dim)\n",
        "    # The -1 in tf.reshape is a placeholder that tells TensorFlow to automatically\n",
        "    # infer that dimension's value based on the total number of elements in the tensor\n",
        "    # -1 is replaced by seq_len by tensorflow\n",
        "\n",
        "  # In TF,Keras - call(self,inputs) is a standard method used inside Layer subclasses\n",
        "  # to define the forward pass of a neural network layer\n",
        "\n",
        "  def call(self, inputs):\n",
        "    query,key,value= inputs\n",
        "    batch_size = tf.shape(query)[0] # (batch_size, seq_len, embed_dim)\n",
        "\n",
        "    query = self.split_heads(self.query_dense(query), batch_size)\n",
        "    key = self.split_heads(self.key_dense(key), batch_size)\n",
        "    value = self.split_heads(self.value_dense(value), batch_size)\n",
        "    attention,_ = self.attention(query,key,value)\n",
        "    attention = tf.transpose(attention, perm= [0,2,1,3])\n",
        "    # before transpose - (batch_size,num_heads,seq_len,projection_dim)\n",
        "    # after transpose - (batch_size, seq_len, num_heads, projection_dim)\n",
        "\n",
        "    concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
        "    # Merge all heads back into a single vector\n",
        "    # (batch_size, seq_len, num_heads, projection_dim) -> (batch_size, seq_len, embed_dim)\n",
        "    return self.combine_heads(concat_attention)\n",
        "\n",
        "class TransformerBlock(Layer):\n",
        "  def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "    super(TransformerBlock,self).__init__()\n",
        "    self.att = MultiHeadAttention(embed_dim, num_heads)\n",
        "    self.ffn = tf.keras.models.Sequential([\n",
        "        Dense(ff_dim, activation= 'relu'),\n",
        "        Dense(embed_dim)\n",
        "    ])\n",
        "    # y = (x-mean)/root(variance + epsilon)\n",
        "    # epsilon ensures that we never divide by zero\n",
        "    # it is small enough to not affect the result but large enough to prevent instability\n",
        "    self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "    self.dropout1 = Dropout(rate)\n",
        "    self.dropout2 = Dropout(rate)\n",
        "    # Note I will create 2 layers, so 2 dropout and layernorm\n",
        "  def call(self, inputs, training):\n",
        "    attn_output = self.att([inputs , inputs, inputs])\n",
        "\n",
        "    # Dropout randomly deactivates some neurons during training to reduce overfitting\n",
        "    # Ensure that dropout is only applied during training and not inference\n",
        "    attn_output = self.dropout1(attn_output, training= training)\n",
        "    out1 = self.layernorm1(inputs + attn_output)\n",
        "    ffn_output = self.ffn(out1)\n",
        "    ffn_output = self.dropout2(ffn_output, training= training)\n",
        "    return self.layernorm2(out1 + ffn_output) # Residual Connection\n",
        "\n",
        "class TokenAndPositionEmbedding(Layer):\n",
        "  def  __init__(self, maxlen, vocab_size, embed_dim):\n",
        "    super(TokenAndPositionEmbedding, self).__init__()\n",
        "    self.token_emb = Embedding(input_dim = vocab_size, output_dim = embed_dim)\n",
        "    self.pos_emb = Embedding(input_dim = maxlen, output_dim = embed_dim)\n",
        "    # The Embedding Layer takes an integer tensor and replaces each integer with an embed_dim sized vector\n",
        "    # example - positions[0,1,2,3] # Keerti Teaches at Educosys\n",
        "    # after embedding - positions = [\n",
        "    #   [0.2, 0.1, 0.3, 0.5, 0.6, 0.9, 0.7, 0.8],  # Position 0\n",
        "    #   [0.4, 0.2, 0.1, 0.6, 0.5, 0.7, 0.9, 0.3],  # Position 1\n",
        "    #   [0.5, 0.3, 0.8, 0.2, 0.7, 0.4, 0.6, 0.1],  # Position 2\n",
        "    #   [0.9, 0.6, 0.2, 0.3, 0.1, 0.8, 0.4, 0.7]   # Position 3\n",
        "    #]\n",
        "\n",
        "    # initial shape of x - (batch_size, seq_len)\n",
        "    # batch_size: Number of sentences in a batch\n",
        "    # seq_len: Number of tokens (words) in each sentence\n",
        "    # Each value in x is an integer index from 0 to vocab_size -1\n",
        "    # after embedding - (batch_size, seq_len, embed_dim)\n",
        "\n",
        "    # example - embed_dim = 8, batch_size = 2\n",
        "    #     x = [\n",
        "    #   [ [0.2, 0.1, 0.4, 0.3, 0.8, 0.7, 0.6, 0.9],  # Token 2\n",
        "    #     [0.5, 0.3, 0.9, 0.1, 0.2, 0.6, 0.8, 0.7],  # Token 5\n",
        "    #     [0.4, 0.9, 0.2, 0.3, 0.1, 0.7, 0.5, 0.6],  # Token 1\n",
        "    #     [0.3, 0.8, 0.6, 0.2, 0.5, 0.9, 0.7, 0.4]   # Token 7\n",
        "    #   ],  # First sentence\n",
        "\n",
        "    #   [ [0.1, 0.6, 0.9, 0.7, 0.3, 0.5, 0.2, 0.8],  # Token 0\n",
        "    #     [0.4, 0.2, 0.3, 0.9, 0.7, 0.5, 0.1, 0.6],  # Token 3\n",
        "    #     [0.8, 0.5, 0.4, 0.1, 0.6, 0.3, 0.2, 0.7],  # Token 8\n",
        "    #     [0.9, 0.3, 0.5, 0.7, 0.8, 0.2, 0.6, 0.1]   # Token 4\n",
        "    #   ]   # Second sentence\n",
        "    # ]\n",
        "\n",
        "  def call(self, x):\n",
        "    # the maximum sequence length the model can handle\n",
        "    maxlen = tf.shape(x)[-1] # sets maxlen to the length of input sequence\n",
        "    positions = tf.range(start=0, limit= maxlen, delta=1) # Generate [0,1,2,..., maxlen-1]\n",
        "    positions = self.pos_emb(positions) # Each position index is mapped to a trainable embedding of shape(maxlen, embed_dim)\n",
        "    x= self.token_emb(x) # Each token ID of x is mapped to an embedding of shape(batch_size, maxlen, embed_dim)\n",
        "    return x + positions\n",
        "\n",
        "    # x has shape (batch_size, maxlen, embed_dim)\n",
        "    # positions have shape(maxlen, embed_dim)\n",
        "    # But maxlen == seq_len, so positions effectively has shape(seq_len,embed_dim)\n",
        "    # Tensorflow broadcasts positions across batch_size, treating it as if it were (1,seq_len, embed_dim)\n",
        "    # This allows element-wise addition between x and position\n"
      ],
      "metadata": {
        "id": "MKas_yJ-jIoF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "906c9942"
      },
      "source": [
        "The line `super(MultiHeadAttention, self).__init__()` is used to call the `__init__` method (the constructor) of the parent class of `MultiHeadAttention`. This is crucial for properly initializing the inherited features and ensuring that all necessary setup from the parent class (`tf.keras.layers.Layer` in this case) is performed before the subclass's own initialization logic runs. It ensures that the `MultiHeadAttention` class behaves correctly as a Keras Layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0665998e"
      },
      "source": [
        "The line `attn_output = self.att([inputs, inputs, inputs])` is calling the `MultiHeadAttention` layer (`self.att`) within the `TransformerBlock`.\n",
        "\n",
        "In a Transformer's self-attention mechanism, the Query, Key, and Value matrices are all derived from the same input sequence. Therefore, `inputs` is passed three times to the `self.att` layer, serving as:\n",
        "\n",
        "*   **Query**: What the attention mechanism is looking for.\n",
        "*   **Key**: What the attention mechanism is comparing against.\n",
        "*   **Value**: The actual information to be extracted and combined.\n",
        "\n",
        "The `attn_output` variable will then hold the result of this self-attention calculation, which is a weighted sum of the Value vectors, emphasizing the most relevant parts of the input sequence based on their relationships."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model the whole architecture , compile and run the training"
      ],
      "metadata": {
        "id": "yqeCQ0lLn83R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Parameters\n",
        "embed_dim =128 # Embedding size\n",
        "num_heads = 4  # Number of attention heads\n",
        "ff_dim = 512   # Feed-forward layer size\n",
        "maxlen= seq_len #here it is 50 defined above\n",
        "\n",
        "# below total words = 6662(see above - basically all tokens in the text)\n",
        "\n",
        "# Build the model\n",
        "inputs = tf.keras.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, total_words, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "print(x.shape)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x, training=True)\n",
        "print(x.shape)\n",
        "x =x[:,-1,:]\n",
        "print(x.shape)\n",
        "x= Dense(total_words, activation='softmax')(x)\n",
        "print(x.shape)\n",
        "model= tf.keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss = 'categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "jOI2Q5W0tnaX",
        "outputId": "c77d02f3-532b-4a17-c491-de8604131f3a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 50, 128)\n",
            "(None, 50, 128)\n",
            "(None, 128)\n",
            "(None, 6663)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer (\u001b[38;5;33mInputLayer\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                          \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " token_and_position_embedding     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)               \u001b[38;5;34m859,264\u001b[0m \n",
              " (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)                                            \n",
              "\n",
              " transformer_block                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)               \u001b[38;5;34m198,272\u001b[0m \n",
              " (\u001b[38;5;33mTransformerBlock\u001b[0m)                                                     \n",
              "\n",
              " get_item (\u001b[38;5;33mGetItem\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " dense_6 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6663\u001b[0m)                  \u001b[38;5;34m859,527\u001b[0m \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
              "\n",
              " input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " token_and_position_embedding     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">859,264</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)                                            \n",
              "\n",
              " transformer_block                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">198,272</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)                                                     \n",
              "\n",
              " get_item (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6663</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">859,527</span> \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,917,063\u001b[0m (7.31 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,917,063</span> (7.31 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,917,063\u001b[0m (7.31 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,917,063</span> (7.31 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X,y, batch_size=32, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iMSMKlS76xh",
        "outputId": "6f246ffa-8ea6-4b15-aa20-478ccd523868"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - accuracy: 0.0812 - loss: 6.5217\n",
            "Epoch 2/10\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.1611 - loss: 5.0805\n",
            "Epoch 3/10\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.2092 - loss: 4.2955\n",
            "Epoch 4/10\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.2527 - loss: 3.6654\n",
            "Epoch 5/10\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.3139 - loss: 3.1324\n",
            "Epoch 6/10\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.3914 - loss: 2.6268\n",
            "Epoch 7/10\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.4684 - loss: 2.2013\n",
            "Epoch 8/10\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.5447 - loss: 1.8386\n",
            "Epoch 9/10\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.6071 - loss: 1.5406\n",
            "Epoch 10/10\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.6632 - loss: 1.2838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, next_words, max_sequence_len):\n",
        "  for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen = max_sequence_len -1, padding='pre')\n",
        "    predicted = model.predict(token_list,verbose=0)\n",
        "    predicted_word = tokenizer.index_word[np.argmax(predicted)]\n",
        "    seed_text+= ' '+ predicted_word\n",
        "  return seed_text\n",
        "\n",
        "# Generate text\n",
        "seed_text = 'harry looked at'\n",
        "generated_text = generate_text(seed_text, next_words=500, max_sequence_len= seq_len+1)\n",
        "print(len(generated_text))\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCVWys8s8Gzy",
        "outputId": "b6c02a06-62c7-49c5-de8f-0cc8a8133ce8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2556\n",
            "harry looked at the dursleys and climbed out into the classroom it was empty except for a large classroom that afternoon harry and ron well done it  harry whispered i think its all worked for me nobody stood up at the moment he couldnt see himself exactly as the other sort of secret i think the ministry of magic has waited norbert  said ron eating their way into the house for ages harry showed them how to get past fluffyneverbut harry suddenly jumped to his feet wherere you going  ron said ron sleepily ive heard of harry  said ron snapes a team  said hermione i mean ive been to talk to the impressed at how ron and hermione had started ron and hermione had started to vibrate if she had always been touched the time  and harry had been more nervous about having a time to find the potters were involved there and i got yeh didnt get him much good day before i think theyd just had a few seconds before he could tell him the potters mrs dursley realized that the potters were involved there was no good and mrs dursley was mrs dursley pleased didnt know the potters if he was forbidden to ask a question he could bear it in the rumor is that lily and james potter are  that theyre  dead  he said that they were in a dead of bushy silver badge on his chest with the wand and gasped lily and jamesi cant believe iti didnt want to believe hed have the little chat and proud of having a little age had clicked the put on the grass reliving the last week harry and his head im not going to do he heard im not going to see what hed heard snapes done it  he said in his family see the other two agreed that they thought they had a week send norbert anything or the dragon or anything  im sorry for my dear life  the dursleys house had four bedrooms one for one and life until christmas dinner the story and believed it of them harry had never seen anything so many things most human beings would choose above all  the trouble is  the magic  the real shock came the boy who had broken down house even though the plump woman said dumbledore so all aboard is it  the leaky cauldron was the freezing in the boat and the boat  so it must be only ones in silence again so this is if im gonna split inter two parties an follow i keep red  the egg an we are the job  yeah thats a goblin  said hagrid glancing at the egg an hed done it  said ron snapes a second later harrys scar its doing very hard on the train home every now the dursleys saying that theres aunt petunia knocking his bacon to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What Is Missing Compared to ChatGPT?\n",
        "\n",
        "- Masked Attention:\n",
        "\n",
        "ChatGPT uses causal masking so that a word cannot see future words during training. Our model uses regular attention, which allows it to see the entire sequence.\n",
        "\n",
        "- Multiple Stacked Transformer Blocks:\n",
        "\n",
        "ChatGPT has many layers (e.g., 12, 24, 96 layers). Our model has only one Transformer block.\n",
        "\n",
        "- Tokenization & Byte-Pair Encoding (BPE):\n",
        "\n",
        "ChatGPT does not use simple tokenization; it uses Byte-Pair Encoding (BPE) or WordPiece for better vocabulary handling. Our model uses basic word tokenization.\n",
        "\n",
        "- Training on Large Datasets:\n",
        "\n",
        "ChatGPT is trained on hundreds of GBs of text. Our model is trained on a single Harry Potter book (very limited).\n",
        "\n",
        "- Decoding Strategies for Text Generation:\n",
        "\n",
        "ChatGPT uses sampling (top-k, nucleus sampling) or beam search to generate text. Our model does not have a decoding strategy."
      ],
      "metadata": {
        "id": "Ux8y9DXZP8A0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RIvmIZIQxZO8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}